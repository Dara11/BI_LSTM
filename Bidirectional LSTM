import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential, load_model
from keras.layers import Dense, Embedding, LSTM, Bidirectional
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
import re

#load the dataset
data = pd.read_excel('humir.xlsx')

tokenizer = Tokenizer(num_words=2000, split=' ')
tokenizer.fit_on_texts(data['Text'])

# fix random seed for reproducibility
np.random.seed(7)
X = tokenizer.texts_to_sequences(data['Text'])
Y = data['Sentiment']


# Split dataset but only keep the top n words, zero the rest
num_words=2000
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

# truncate and pad input sequences
max_review_length = 200
X_train = pad_sequences(X_train, maxlen=max_review_length)
X_test = pad_sequences(X_test, maxlen=max_review_length)

#create the model
from keras.layers import Dropout
embedding_vecor_length = 128
model = Sequential()
model.add(Embedding(num_words, embedding_vecor_length, input_length = max_review_length))
model.add(Dropout(0.2))
model.add(Bidirectional(LSTM(50)))
model.add(Dropout(0.2))
model.add(Dense(1,activation = 'sigmoid'))
model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
print(model.summary())

model.fit(X_train,Y_train, epochs=5, batch_size=25)

#evaluate the model
scores = model.evaluate(X_test, Y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

# Predicting the Test set results
Y_pred = model.predict(X_test)
Y_pred = (Y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix,classification_report
print(confusion_matrix(Y_test, Y_pred))
print('\n')
print(classification_report(Y_test,Y_pred))

# Part 4 Evaluating, Improving and Tuning the NN

# Evaluating the NN
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Bidirectional
from keras.layers import Dropout
embedding_vecor_length = 128
def build_model():
    model = Sequential()
    model.add(Embedding(num_words, embedding_vecor_length, input_length = max_review_length))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(50)))
    model.add(Dropout(0.2))
    model.add(Dense(1,activation = 'sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    return model
model = KerasClassifier(build_fn = build_model, batch_size = 25, epochs = 3)
accuracies = cross_val_score(estimator = model, X = X_train, y = Y_train, cv = 10, n_jobs = 1)
mean = accuracies.mean()
variance = accuracies.std()

# Tuning the NN
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Bidirectional,Dropout
embedding_vecor_length = 128
def build_model(optimizer):
    model = Sequential()
    model.add(Embedding(num_words, embedding_vecor_length, input_length = max_review_length))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(50)))
    model.add(Dropout(0.2))
    model.add(Dense(1,activation = 'sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])
    return model
model = KerasClassifier(build_fn = build_model)
parameters = {'batch_size': [25, 32],
              'epochs': [1,2],
              'optimizer': ['adam', 'rmsprop']}
grid_search = GridSearchCV(estimator = model,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10)
grid_search = grid_search.fit(X_train, Y_train)
best_parameters = grid_search.best_params_
best_accuracy = grid_search.best_score_
